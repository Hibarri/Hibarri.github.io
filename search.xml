<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>pytorch</title>
      <link href="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><code>˙Ꙫ˙</code></p><span id="more"></span><h2 id="神经网络基本骨架nn-Module"><a href="#神经网络基本骨架nn-Module" class="headerlink" title="神经网络基本骨架nn.Module"></a>神经网络基本骨架<code>nn.Module</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):  <span class="comment"># 继承nn.Module类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  <span class="comment"># 初始化</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conc2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)  <span class="comment"># 卷积</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  <span class="comment"># 前向传播 输入x</span></span><br><span class="line">        x = F.relu(self.conv1(x))  <span class="comment"># 卷积+非线性处理</span></span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))  <span class="comment"># 又一次卷积+非线性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># e.g.</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">my_model = My_Model()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">output = my_model(x)</span><br></pre></td></tr></table></figure><h3 id="卷积层-Convolution-Layers"><a href="#卷积层-Convolution-Layers" class="headerlink" title="卷积层 Convolution Layers"></a>卷积层 <code>Convolution Layers</code></h3><p><code>nn.Conv2d</code> 2维卷积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 一般前五个参数需要自己设置</span></span><br></pre></td></tr></table></figure><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels in the input image</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – Size of the convolving kernel   在训练过程中不断调整</li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – Padding added to all four sides of the input. Default: 0</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – <code>&#39;zeros&#39;</code>, <code>&#39;reflect&#39;</code>, <code>&#39;replicate&#39;</code> or <code>&#39;circular&#39;</code>. Default: <code>&#39;zeros&#39;</code></li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></li></ul><p>Shape计算</p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221106205832895.png" alt="image-20221106205832895"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.Totensor(), download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>) <span class="comment"># batch_seze一次取64张图片</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>) <span class="comment"># 输出3通道 期望输出6通道 kernel大小3×3 横向纵向每次走一步</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">my_model = My_Model()</span><br><span class="line"><span class="comment"># print(my_model)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># writer = SummaryWriter(&quot;../logs&quot;) # 可用tensorboard直观显示</span></span><br><span class="line"><span class="comment"># step = 0</span></span><br><span class="line"><span class="comment"># 查看每张图像</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = my_model(imgs)</span><br><span class="line">    <span class="comment"># print(output.shape)  </span></span><br><span class="line">    <span class="comment"># writer.add_images(&quot;input&quot;, imgs, step)</span></span><br><span class="line">    <span class="comment"># torch.reshape(output, (-1,3,30,30))</span></span><br><span class="line">    <span class="comment"># writer.add_images()</span></span><br><span class="line">    <span class="comment"># step ++;</span></span><br><span class="line">    <span class="comment"># 终端pytorch环境下输入tensorboard --logdir=logs</span></span><br></pre></td></tr></table></figure><p>常用卷积层</p><p>VGG16  注意设置的padding大小</p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221106210116509.png" alt="image-20221106210116509"></p><h3 id="池化层-Pooling-layers"><a href="#池化层-Pooling-layers" class="headerlink" title="池化层 Pooling layers"></a>池化层 <code>Pooling layers</code></h3><p><code>MaxPool2d</code>最大池化</p><p>最大池化MaxPool </p><p>即取局部值最大的点 下采样</p><p>保留输入特征同时减小数据量</p><p>(MaxUnpool 上采样)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># stride默认值为kernel_size</span></span><br><span class="line"><span class="comment"># ceil_mode 向上取整 floor_mode 向下取整</span></span><br><span class="line"><span class="comment"># 一般只需要设置kernel_size</span></span><br></pre></td></tr></table></figure><p>在输入图像上每次取池化核大小范围内的最大值（步长默认为核大小）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># input必须为四维 (N(batchsize),Channel,Height,Width)</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))<span class="comment"># batchsize=-1(-1时自动计算),channel=1,5×5</span></span><br><span class="line"><span class="comment"># print(input.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">my_model = My_Model()</span><br><span class="line">output = my_model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="非线性激活-Non-linear-Activations"><a href="#非线性激活-Non-linear-Activations" class="headerlink" title="非线性激活 Non-linear Activations"></a>非线性激活 <code>Non-linear Activations</code></h3><p>常用非线性激活</p><p><code>ReLU</code>    <code>SIGMOID</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以ReLU为例 input&lt;0 output=0, input&gt;=0 output=input</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, -<span class="number">0.5</span>],</span><br><span class="line">                      [-<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        self.relu1 = ReLU() <span class="comment"># inplace=True将input值替换为output 默认False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>)</span><br><span class="line">    output = self.relu1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">my_model = My_Model()</span><br><span class="line">output = my_model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="线性层-Linear-Layers"><a href="#线性层-Linear-Layers" class="headerlink" title="线性层 Linear Layers"></a>线性层 <code>Linear Layers</code></h3><h3 id="Normalization-Layers"><a href="#Normalization-Layers" class="headerlink" title="Normalization Layers"></a><code>Normalization Layers</code></h3><p><code>BatchNorm2d</code>  防止过拟合</p><h3 id="Dropout-Layers"><a href="#Dropout-Layers" class="headerlink" title="Dropout Layers"></a><code>Dropout Layers</code></h3><p>随机失活  防止过拟合</p><h3 id="Sequential-amp-amp-Sifar-model-structure"><a href="#Sequential-amp-amp-Sifar-model-structure" class="headerlink" title="Sequential &amp;&amp; Sifar model structure"></a><code>Sequential</code> &amp;&amp; <code>Sifar model structure</code></h3><p>以SIFAR10数据集及其模型为例</p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/sifar10.webp" alt="sifar10"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">my_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(my_Model, self).__init__()</span><br><span class="line">        <span class="comment"># self.conv1 = Conv2d(3, 32, 5, padding=2)  # in_channel=3 out_channel=32 kernel=5</span></span><br><span class="line">        <span class="comment"># # 求出padding=2和stride=1</span></span><br><span class="line">        <span class="comment"># self.maxpool1 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.conv2 = Conv2d(32, 32, 5, padding=2)</span></span><br><span class="line">        <span class="comment"># self.maxpool2 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.conv3 = Conv2d(32, 64, 5, padding=2)</span></span><br><span class="line">        <span class="comment"># self.maxpool3 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># self.flatten = Flatten()</span></span><br><span class="line">        <span class="comment"># self.linear1 = Linear(1024, 64)  # 线性层 1024=64x4x4</span></span><br><span class="line">        <span class="comment"># self.linear2 = Linear(64, 10) # 10个类别 最终输出10</span></span><br><span class="line"></span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = self.conv1(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool1(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv2(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool2(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv3(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool3(x)</span></span><br><span class="line">        <span class="comment"># x = self.flatten(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear1(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear2(x)</span></span><br><span class="line"></span><br><span class="line">        x= self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">my_model = my_Model()</span><br><span class="line"><span class="built_in">print</span>(my_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查网络正确性</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = my_model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ten sorboard可视化</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_seq&quot;</span>)</span><br><span class="line">writer.add_graph(my_model, <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cmd 该环境下</span></span><br><span class="line">tensorboard --logdir=logs_seq --port=<span class="number">6007</span></span><br></pre></td></tr></table></figure><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221201111252035-16698643744131.png" alt="image-20221201111252035"></p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221201111535140-16698645377943.png" alt="image-20221201111535140"></p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221201111604633.png" alt="image-20221201111604633"></p><h3 id="损失函数-amp-amp-反向传播-amp-amp-优化器"><a href="#损失函数-amp-amp-反向传播-amp-amp-优化器" class="headerlink" title="损失函数 &amp;&amp; 反向传播 &amp;&amp; 优化器"></a>损失函数 &amp;&amp; 反向传播 &amp;&amp; 优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以CIFAR10数据集为例 使用上述模型</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵计算loss</span></span><br><span class="line">my_model = my_Model()</span><br><span class="line">optim = torch.optim.SGD(my_model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 优化器optimizer 随机梯度下降 lr学习速率</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):  <span class="comment"># 遍历20轮数据集</span></span><br><span class="line">    running_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = my_model(imgs)</span><br><span class="line">        result_loss = loss(output, targets)</span><br><span class="line">        <span class="comment"># print(result_loss)</span></span><br><span class="line">        optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        result_loss.backward()  <span class="comment"># 求出每个节点的梯度</span></span><br><span class="line">        optim.step()  <span class="comment"># 选用合适的优化器 梯度下降</span></span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss)  <span class="comment"># 打印每一轮的loss之和</span></span><br></pre></td></tr></table></figure><p>报错;</p><p><img src="/2022/11/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20221201202915739.png" alt="image-20221201202915739"></p><p>解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br></pre></td></tr></table></figure><h3 id="现有模型使用及修改"><a href="#现有模型使用及修改" class="headerlink" title="现有模型使用及修改"></a>现有模型使用及修改</h3><p>VGG16模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data = torchvision.datasets.CIFAR10(&#x27;../data&#x27;, train=True, transform=torchvision.transforms.ToTensor(), download=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给VGG16添加一个线性层 input=1000 output=10</span></span><br><span class="line">vgg16_false.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment">#修改VGG16最后一个线性层(第七层)</span></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure><h3 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型保存与加载</span></span><br><span class="line"><span class="comment"># 保存方式1</span></span><br><span class="line">torch.save(vgg16_false, <span class="string">&quot;vgg16_1.pth&quot;</span>)  <span class="comment"># 保存模型结构+参数</span></span><br><span class="line"><span class="comment"># 对应加载模型方式</span></span><br><span class="line">model_1 = torch.load(<span class="string">&quot;vgg16_1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式2(官方推荐)</span></span><br><span class="line">torch.save(vgg16_false.state_dict(), <span class="string">&quot;vgg16_2.pth&quot;</span>)  <span class="comment"># 仅保存模型参数</span></span><br><span class="line"><span class="comment"># 对应加载模型方式</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_1.pth&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br></pre></td></tr></table></figure><h3 id="完整模型训练套路"><a href="#完整模型训练套路" class="headerlink" title="完整模型训练套路"></a>完整模型训练套路</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> * <span class="comment"># 引入model</span></span><br><span class="line"><span class="comment"># import ssl</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span></span><br><span class="line"><span class="comment"># import torch</span></span><br><span class="line"><span class="comment"># import torchvision</span></span><br><span class="line"><span class="comment"># from torch.utils.tensorboard import SummaryWriter</span></span><br><span class="line"><span class="comment"># from torch.utils.data import DataLoader</span></span><br><span class="line"></span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练集长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用dataloader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">my_model = my_Model()</span><br><span class="line">my_model = my_model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的参数</span></span><br><span class="line"><span class="comment"># 记录训练次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard 可视化loss</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_train&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------第&#123;&#125;轮训练开始------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    my_model.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        outputs = my_model(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;, loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item(), total_train_step))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    my_model.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 只测试 不对梯度进行调整</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">            outputs = my_model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss = loss + total_test_loss</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">        total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存每一轮的模型</span></span><br><span class="line">    <span class="comment"># torch.save(my_model, &quot;my_model_&#123;&#125;.pth&quot;.format(i+1))</span></span><br><span class="line">    torch.save(my_model.state_dict(), <span class="string">&quot;my_model_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>看这里啦</p><p><a href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a></p><p><a href="https://www.bilibili.com/video/BV1hE411t7RN?p=18&amp;spm_id_from=pageDriver&amp;vd_source=3def6347655903bff36de76eff34d8eb">https://www.bilibili.com/video/BV1hE411t7RN?p=18&amp;spm_id_from=pageDriver&amp;vd_source=3def6347655903bff36de76eff34d8eb</a></p><p><code>google colab</code></p></blockquote><hr><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>龟兔赛跑</title>
      <link href="/2022/10/17/%E9%BE%9F%E5%85%94%E8%B5%9B%E8%B7%91%E7%AE%97%E6%B3%95/"/>
      <url>/2022/10/17/%E9%BE%9F%E5%85%94%E8%B5%9B%E8%B7%91%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p><code>Floyd判圈算法</code>即<code>龟兔赛跑算法</code></p><span id="more"></span><p>​    假想「乌龟」和「兔子」在链表上移动，「兔子」跑得快，「乌龟」跑得慢。当「乌龟」和「兔子」从链表上的同一个节点开始移动时，如果该链表中没有环，那么「兔子」将一直处于「乌龟」的前方；如果该链表中有环，那么「兔子」会先于「乌龟」进入环，并且一直在环内移动。等到「乌龟」进入环时，由于「兔子」的速度快，它一定会在某个时刻与乌龟相遇，即套了「乌龟」若干圈。</p><p><code>力扣141.环形链表</code>给你一个链表的头节点<code>head</code>，判断链表中是否有环。若链表中存在环，则返回<code>true</code>，否则返回<code>false</code>。</p><p>解：</p><p>​    定义快慢两个指针，慢指针每次移动一步，快指针每次移动两步。初始时，慢指针位于head处，快指针位于head-&gt;next处。在移动过程中，快指针追上满指针，则有环，否则快指针到达链表尾部，则无环。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">hasCycle</span><span class="params">(ListNode* head)</span></span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head-&gt;next == <span class="literal">nullptr</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            ListNode* slow = head;</span><br><span class="line">            ListNode* fast = head-&gt;next;</span><br><span class="line">            <span class="keyword">while</span>(slow!=fase)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(fast==<span class="literal">nullptr</span>||fast-&gt;next==<span class="literal">nullptr</span>)</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                </span><br><span class="line">                slow = slow-&gt;next;</span><br><span class="line">                fast = fast-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://leetcode.cn/problems/linked-list-cycle/solution/huan-xing-lian-biao-by-leetcode-solution/">https://leetcode.cn/problems/linked-list-cycle/solution/huan-xing-lian-biao-by-leetcode-solution/</a></p></blockquote><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> 链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>今天的也不好笑吗</title>
      <link href="/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/"/>
      <url>/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>每天更一个冷笑话</p><p>让<span style="color:pink">猫猫</span>笑一笑</p></blockquote><p>好吧  可能做不到每天  可能也不好笑</p><p>但我保证 一定很冷！！</p><p>ت</p><span id="more"></span><hr><p><strong>2024.3.8</strong></p><p>小明愤怒的打开水龙头</p><p>因为开水龙头烫着他了</p><p><strong>2023.2.6</strong></p><p>Teacher: “You missed school yesterday!”</p><p>Student: “To tell you the truth, I didn’t really missed it.”</p><p><strong>2023.1.27</strong></p><p>一只菠萝去理发</p><p>人有点多</p><p>她排很久的队才轮到她</p><p>可理发师半天都没帮她理发</p><p>她就很委屈的说：</p><p>”你理理我吧“</p><p><strong>2023.1.22</strong></p><p>小牛在海边散步</p><p>突然一道浪劈了过来</p><p>打一词语</p><blockquote class="blockquote-center">            <i class="fa fa-quote-left"></i>            <font color="Plum" size="4">海批牛耶！！</font>            <i class="fa fa-quote-right"></i>          </blockquote><p><strong>2023.1.7</strong></p><p>—Which runs faster, hot or cold？</p><p>—Hot. Because we all can catch a cold.</p><p><strong>2023.1.1</strong> <code>Happy New Year</code></p><p>一群碳基生物因为他们居住的星球公转了一圈而高兴</p><p>而我不一样</p><p>我每天都傻乐</p><p><strong>2022.12.25</strong> <code>Merry Christmas</code></p><p>也有人给我画圣诞树啦<code>(ﾉ∇︎〃 )</code></p><p><img src="/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/image-20221225193029950.png" alt="image-20221225193029950"></p><p><strong>2022.12.21</strong></p><p><img src="/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/image-20221221112111278.png" alt="image-20221221112111278"></p><p><strong>2022.12.13</strong></p><p>今天！ 狠狠地吃了一口狗粮！！<code>ꐦ≖ ≖</code></p><p>下午麻麻在弯着腰收拾东西</p><p>我趁机朝着麻麻的腚上就是温柔的一掌</p><p>我妈看了看我</p><p>扭头朝着我爸：“老公~ 你女儿打我屁股呜”</p><p>。。。。打小报告的怎么还</p><p><strong>2022.12.11</strong></p><p>世界上所有东西都会骗我</p><p>只有披萨不会</p><p>因为披萨有六片八片十二片就是没有七片</p><p>所以宝  请我吃个🍕好不好~</p><p><strong>2022.12.9</strong></p><p>今天 </p><p>更一个羌羌羌羌</p><p>羌族小煞</p><p>⠀⠀⠀⠀ ⣶⣿⣿⣿⣿⣿⣿⣿⣿⣄<br>⠀⠀⣰⣿⣿⣿⡿⠟⠛⠛⠛⠿⣿⣿⣿<br>⠀⠀⢸⣿⣿⣥⣤⠀⠀⠀⣀⣀⠀⠘⣿⡏<br>⠀⠀⠈⡟⠿⠶⠂⠀⠀⠀⠶⠆⠀⠀⣿⠃<br>⠀⠀⠀⣷⡀⠀⣀⣀⠀⠀⠀⠀⠀⠀⠂<br>⠀⠀⠀⠀⢻⣦⣤⣥⣀<br>⠀⠀⠀⠀⠀⣿⣿⣛⡁⠀⠀⠀⠀⡀<br>⠀⣀⣴⠃⠘⣿⣿⣿⡿⠀⠀⠀⠀⣼⣿⣶⣄<br>⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⢀⣠⣾⣿⣿⣿⣿</p><p><strong>2022.12.6</strong></p><p>小动物们一起接受挑战</p><p>小猪小兔小狗小猫要同步按下按钮才算挑战成功</p><p>但是因为小猪慢半拍</p><p>它们失败了</p><p>大家很生气的说：</p><p>“猪你晚按啦！！”</p><p><strong>2022.12.1</strong></p><p>一张板凳用英语说：abandon</p><p><strong>2022.11.27</strong></p><p><img src="/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/image-20221127110520408-16695183250351.png" alt="image-20221127110520408"></p><p><strong>2022.11.25</strong></p><p>有一天 我拦下一辆出租车</p><p>问司机：“从这儿到机场要多久”</p><p>司机：“要很久的”</p><p>我：“起码要多久”</p><p>司机：“骑马要更久”</p><p><strong>2022.11.24</strong></p><p>“一只蚯蚓被切成两段</p><p>等这两段长大变成两只蚯蚓后</p><p>它们之间是什么关系？”</p><p>“熟悉的陌生蚓”</p><p><strong>2022.11.21</strong></p><p>我对生活抱怨好苦 T^T</p><p>生活说给我加点糖</p><p>我：什么糖</p><p>生活：一点荒唐</p><p><strong>2022.11.16</strong></p><p>今天我不问鸭子 我问麻雀</p><p>“想扎什么发型”</p><p>“啾啾”</p><p>“揪揪就揪揪！”</p><p><strong>2022.11.15</strong></p><p>我问鸭子</p><p>我的谐音梗尬不尬</p><p>鸭子： “嘎”</p><p><strong>2022.11.13</strong></p><p>跟你们说件事你们可能不信</p><p>我不说了</p><p><strong>2022.11.11</strong></p><p>这个世界上真的有龙</p><p>我在屋里玩电脑</p><p>我妈在外面叫我吃饭</p><p>叫了三遍以后</p><p>她就推开门问我</p><p>你是不是龙？</p><p><strong>2022.11.8</strong></p><p>熊猫点外卖</p><p>​                ——笋到家了！</p><p><strong>2022.11.7</strong></p><p>“知道什么动物不挑食吗”</p><p>“鸡”</p><p>饥不择食哈哈</p><p><strong>2022.11.6</strong></p><p>“你买的什么书”</p><p>“编程”</p><p>“c++还是java”</p><p>“沈从文”</p><p>是《边城》啦！</p><p><strong>2022.11.3</strong></p><p>王八走读</p><p>​        ——憋不住笑了</p><p>（鳖不住校了）</p><p><strong>2022.10.26</strong></p><p>世界上总有许多出人意料的事情</p><p>比如</p><p>你以为我会举个例子</p><p><strong>2022.10.25</strong></p><p>yh非常胆小</p><p>但又宇宙炒鸡无敌喜欢看恐怖片</p><p>他每次呢只敢在大白天看</p><p>这一天 杰哥给了yh一张光盘</p><p>神秘地告诉他一定要在今天的半夜看</p><p>否则会带来厄运</p><p>夜色渐渐深了</p><p>外面的风呜呜刮着</p><p>yh小心翼翼的把光盘插入</p><p>紧张的心脏怦怦怦怦怦怦跳</p><p>电视屏幕一闪一闪一闪一闪</p><p>零点了</p><p>突然出现了一道白光</p><p>一个看不清脸的长发人影逐渐靠近</p><p>嘴里还念念有词</p><p>yh害怕极了</p><p>抱着枕头蜷缩在沙发里</p><p>人影越来越近</p><p>yh捂住了眼睛</p><p>但过了很久都没有声音</p><p>他疑惑地抬头一瞅</p><p>长发人影唰的一下冲过来朝他大喊</p><p>“ <span style="color:#F0BBFF">哥，二十一岁生日快乐啦!</span> ”</p><p><strong>2022.10.24</strong></p><p>“你看见我的紫薇了吗？”</p><p>“你的嘴不就在你脸上吗？？”</p><p><strong>2022.10.23</strong></p><p>“汽车会飞 打一饮料”</p><p>“是咖啡啦”</p><blockquote><p>从wyy那儿得来的冷笑话哈哈</p></blockquote><p><strong>2022.10.21</strong></p><p>“知道理发师出门要带什么嘛”</p><p>“带水</p><p> 因为 托尼带水”</p><p><strong>2022.10.20</strong></p><p>知道小公鸡为什么不能去学校上学嘛</p><p>因为学校得要一寸免冠照片</p><p><strong>2022.10.17</strong></p><p>今天</p><p>写一个递归</p><p><img src="/2022/10/05/%E4%BB%8A%E5%A4%A9%E4%B9%9F%E8%A6%81%E7%AC%91%E4%B8%80%E7%AC%91/递归.png" alt="递归"></p><p><strong>2022.10.16</strong></p><p>过年了</p><p>牛魔王犯错 铁扇公主不停地批评他</p><p>孙悟空实在看不下去了 对铁扇公主说：</p><p> “嫂嫂，还批牛爷？”</p><p>铁扇公主愣了一下 说：</p><p>“谢谢谢谢”</p><p><strong>2022.10.15</strong></p><p>我一睁开眼睛就亮</p><p>一闭上眼睛就暗</p><p>会不会我也是一个冰箱呢</p><p><strong>2022.10.14</strong></p><p>“哪种动物最安静”</p><p>“是猩猩</p><p>因为它喜欢敲咪咪”</p><p><strong>2022.10.12</strong></p><p>“我有一份让人惊讶的工作”</p><p>“什么工作”</p><p>“挖藕”</p><p><strong>2022.10.11</strong></p><p>今天不想写冷笑话了</p><p>想写几句好可爱的歌词！！</p><blockquote class="blockquote-center">            <i class="fa fa-quote-left"></i>            <p><span style="color:#F0BBFF">可能是月亮不会眨眼<br>星星不会说话<br>让你觉得孤单啦<br>可能是太阳熬夜太晚<br>云朵们都太懒<br>忘了帮你编童话<br>可能是你所谓的转角<br>是其他人的直线<br>所以没法给你个回答</span></p>            <i class="fa fa-quote-right"></i>          </blockquote><p><strong>2022.10.10</strong></p><p>一天呢</p><p>开心和高兴约好12点去吃午饭</p><p>然后开心就开始打游戏</p><p>让高兴到点叫他</p><p>结果两个人都忘记了</p><p>高兴觉得肚子很饿</p><p>一看时间都一点了</p><p>于是他就喊</p><p>“开心一点啦</p><p>开心一点啦！”</p><p><strong>2022.10.9</strong></p><p>张飞和刘备骑马骑到悬崖边上</p><p>刘备喊：“你快勒马！”</p><p>张飞答：“我很快乐！”</p><p><strong>2022.10.8</strong></p><p>有一只大象</p><p>在吃冰激凌</p><p>但是吃着吃就吐了</p><p>因为</p><p>象腻了！</p><p><strong>2022.10.7</strong></p><p>“Do u have a girlfriend?”</p><p>“Yeah, she is from another nation.”</p><p>“Which nation?”</p><p>“Imagination.”</p><p><strong>2022.10.6</strong></p><p>有一天呢</p><p>我和yh吵架</p><p>yh非常生气</p><p>气的他夺门而出</p><p>我一看 这哪行啊 我得追回来</p><p>于是我追着他跑了好几条街</p><p>终于把家门给拿回来了</p><p><strong>2022.10.5</strong></p><p>从前有一只小绵羊</p><p>某一天心血来潮去剪了羊毛</p><p>就再也睡不着了</p><p>因为它 失绵啦</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
        <tags>
            
            <tag> 心情 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微机原理</title>
      <link href="/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
      <url>/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p><code>(꒪⌓꒪)</code></p><span id="more"></span><p>1字节 = 8位</p><p>字节是最小的寻<strong>址</strong>单位！</p><p>1字 = 2字节 = 16位</p><p>WORD 字 16位</p><p>DWORD 双字 32位</p><h2 id="Intel-x86微处理器的寄存器结构"><a href="#Intel-x86微处理器的寄存器结构" class="headerlink" title="Intel x86微处理器的寄存器结构"></a>Intel x86微处理器的寄存器结构</h2><p><img src="/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/image-20221005205008112-16660610059285.png" alt="image-20221005205008112">通用寄存器：主要用于<code>算术运算</code>和<code>数据传送</code></p><p><img src="/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/image-20221008210013032.png" alt="image-20221008210013032"></p><p>AX通常称为累加器</p><p>BX基地址寄存器，作为存储器指针来用</p><p>CX计数寄存器</p><p>DX数据寄存器</p><p>BP常用来寻址堆栈而不是数据段</p><p><strong>标志位</strong>：CF（进位）、ZF（零）、SF（符号）、OF（溢出）、AF（辅助进位）、PF（奇偶）</p><h2 id="16位系统寻址方式"><a href="#16位系统寻址方式" class="headerlink" title="16位系统寻址方式"></a>16位系统寻址方式</h2><h4 id="1-立即寻址"><a href="#1-立即寻址" class="headerlink" title="1.立即寻址"></a>1.立即寻址</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, im  ; im:立即数   MOV AX, 43H</span><br><span class="line">MOV AX, @DATA  ; 在编译时返回整数常量值</span><br><span class="line">MOV SI, OFFSET ARRAY  ;??</span><br></pre></td></tr></table></figure><h4 id="2-直接寻址"><a href="#2-直接寻址" class="headerlink" title="2.直接寻址"></a>2.直接寻址</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, [ARRAY]     ;加上方括号</span><br><span class="line">SUB AX, [ARRAY+2]</span><br><span class="line">MOV AX, DS:[2000H]  ;设DS=3000H 物理地址：3000H×16(即左移4位变为20位)+2000H  偏移地址2000H由指令直接给出  不加方括号立即寻址 加上直接寻址</span><br></pre></td></tr></table></figure><h4 id="3-寄存器寻址"><a href="#3-寄存器寻址" class="headerlink" title="3.寄存器寻址"></a>3.寄存器寻址</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MOV DS, AX</span><br></pre></td></tr></table></figure><h4 id="4-寄存器间接寻址"><a href="#4-寄存器间接寻址" class="headerlink" title="4.寄存器间接寻址"></a>4.寄存器间接寻址</h4><p>操作数放在存储器中，操作数的16位段内偏移地址存放在SI、DI、BP、BX这四个寄存器之一中</p><ul><li><p>若以SI、DI、BX间接寻址，则操作数存放在现行数据段中，此时，数据段寄存器<code>DS</code>的内容加上<code>SI、DI、BX</code>中的16位段内偏移地址，即得操作数地址</p></li><li><p>若以BP间接寻址，操作数存放在堆栈段区域中，此时，堆栈段寄存器<code>SS</code>的内容加上<code>BP</code>中的16位段内偏移地址，即得操作数地址</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, [SI]  ;设DS=2000H，SI=1000H  则操作数地址=21000H</span><br><span class="line">MOV AX, [BP]  ;设SS=3000H，BP=2000H  则操作数地址=32000H</span><br></pre></td></tr></table></figure><p><strong>DS、SS需左移四位</strong></p><h4 id="5-寄存器相对寻址"><a href="#5-寄存器相对寻址" class="headerlink" title="5.寄存器相对寻址"></a>5.寄存器相对寻址</h4><p>操作数存放在存储器中，操作数地址是由<code>段寄存器内容(SI、DI、BX对应DS，BP对应SS)</code>加上<code>SI、DI、BX、BP</code>其中之一的内容，再加上由<code>指令中所指出的8位或16位带符号相对地址偏移量</code>而得到的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, DISP[SI]  ;操作数地址为DISP+SI+DS(DS需左移4位)</span><br><span class="line">ADD AX, ARRAY[SI] </span><br><span class="line">MOV AX, [BX+6]</span><br></pre></td></tr></table></figure><h4 id="6-基址变址寻址"><a href="#6-基址变址寻址" class="headerlink" title="6.基址变址寻址"></a>6.基址变址寻址</h4><p>在8086/8088中，通常把<strong>BX和BP作为基址寄存器</strong>，而把<strong>SI、DI作为变址寄存器</strong>，将这两种寄存器联合起来进行的寻址就称为基址+变址寻址。</p><p>此时，操作数的地址为<code>段寄存器的内容（DS或SS BX对应DS BP对应SS）</code>加上<code>基址寄存器的内容（BX或BP）</code>，再加上<code>变址寄存器内容（SI或DI）</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, [BX][SI]  ;操作数地址为BX+SI+DS(DS&lt;&lt;4)</span><br></pre></td></tr></table></figure><h4 id="7-基址变址相对寻址"><a href="#7-基址变址相对寻址" class="headerlink" title="7.基址变址相对寻址"></a>7.基址变址相对寻址</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MOV AX, DISP[BX][SI]  ;BX+SI+DISP+DS(DS&lt;&lt;4)</span><br></pre></td></tr></table></figure><h4 id="8-PC相对寻址"><a href="#8-PC相对寻址" class="headerlink" title="8.PC相对寻址"></a>8.PC相对寻址</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOOP L1   ; PC相对寻址</span><br></pre></td></tr></table></figure><h4 id="9-隐含寻址"><a href="#9-隐含寻址" class="headerlink" title="9.隐含寻址"></a>9.隐含寻址</h4><p>在有些指令中，不仅包含有操作码的信息，而且还隐含了操作数地址的信息。</p><h2 id="汇编语言"><a href="#汇编语言" class="headerlink" title="汇编语言"></a>汇编语言</h2><p>大小写不敏感 标识符不能以数字开头</p><p>R 寄存器</p><p>M 内存</p><p>AL 寄存器低八位</p><p>h 十六进制           b 二进制              q/o八进制               r 编码实数            d 十进制</p><p>注意：<strong>0</strong>A3h</p><h3 id="一、定义数据"><a href="#一、定义数据" class="headerlink" title="一、定义数据"></a>一、定义数据</h3><p><strong>BYTE</strong> 8位无符号<strong>整数</strong>                       SBYTE 8位有符号整数</p><p><strong>WORD</strong> 16位无符号整数                  SWORD 16位有符号整数</p><p><strong>DWORD</strong> 32位无符号整数                SDWORD 32位有符号整数</p><p>FWORD 48位整数                 </p><p>QWORD 64位整数</p><p>db 字节 8位</p><p>dw 字    = 2字节  16位</p><p>dd 双字   = 4字节  </p><p><strong>小端存储</strong></p><p><img src="/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/image-20221014234431129.png" alt="image-20221014234431129"></p><p>符号常量不占用任何实际的存储空间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">; 计算数组和字符串的大小</span><br><span class="line">list1 BYTE 10,20,30,40</span><br><span class="line">List1Size = ($ - list1)  ; $代表当前偏移地址</span><br><span class="line">myString BYTE &quot;This is a long string,&quot;</span><br><span class="line">         BYTE &quot; Containing any number&quot;</span><br><span class="line">         BYTE &quot; of characters&quot;,0dh,0ah</span><br><span class="line">MyString_len = ($ - myString)</span><br><span class="line">list2 WORD 1000h,2000h,3000h,4000h</span><br><span class="line">List2Size = ($ - list2)/2</span><br></pre></td></tr></table></figure><h3 id="二、和数据相关的操作符和伪指令"><a href="#二、和数据相关的操作符和伪指令" class="headerlink" title="二、和数据相关的操作符和伪指令"></a>二、和数据相关的操作符和伪指令</h3><h4 id="预定义符号"><a href="#预定义符号" class="headerlink" title="预定义符号$"></a>预定义符号<code>$</code></h4><ul><li><p>表示当前这条指令在代码段中的偏移量</p></li><li><p>表示字符串结束   0dh,0ah换行</p></li></ul><h4 id="OFFSET操作符"><a href="#OFFSET操作符" class="headerlink" title="OFFSET操作符"></a><code>OFFSET</code>操作符</h4><p>返回数据标号的偏移地址（标号据数据段开始的距离，以字节为单位）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">bVal  BYTE  ?      ; 假设bVal位于00303000h处</span><br><span class="line">wVal  WORD  ?</span><br><span class="line">dVal1 DWORD ?</span><br><span class="line">dVal2 DWORD ?</span><br><span class="line">.code</span><br><span class="line">……</span><br><span class="line">mov esi,OFFSET bVal; ESI = 00303000</span><br><span class="line">mov esi,OFFSET wVal; ESI = 00303001</span><br><span class="line">mov esi,OFFSET dVal1; ESI = 00303003</span><br><span class="line">mov esi,OFFSET dVal2; ESI = 00303007</span><br><span class="line">mov esi,OFFSET bVal + 1</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="PTR操作符"><a href="#PTR操作符" class="headerlink" title="PTR操作符"></a><code>PTR</code>操作符</h4><p>用来重载操作数的默认尺寸</p><p>必须和以下标准数据类型联合使用：BYTE，SBYTE，WORD，SWORD，DWORD，SDWORD，FWORD，QWORD，TBYTE</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">myDouble DWORD 12345678h        ;小端存储</span><br><span class="line">.code</span><br><span class="line">mov ax, myDouble; 错误!</span><br><span class="line">mov ax, WORD PTR myDouble; ax = 5678h</span><br><span class="line">mov ax, WORD PTR [myDouble+2]; ax = 1234h</span><br><span class="line">mov bl, BYTE PTR myDouble; bl = 78h</span><br></pre></td></tr></table></figure><h4 id="TYPE操作符"><a href="#TYPE操作符" class="headerlink" title="TYPE操作符"></a><code>TYPE</code>操作符</h4><p>返回按字节计算的变量的单个元素的大小</p><h4 id="DUP伪指令"><a href="#DUP伪指令" class="headerlink" title="DUP伪指令"></a><code>DUP</code>伪指令</h4><p>duplicate 重复初始化数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str1 db 10 dup(&#x27;!@#&#x27;)  ;十个&#x27;!@#&#x27;</span><br></pre></td></tr></table></figure><h4 id="LENGTHOF操作符"><a href="#LENGTHOF操作符" class="headerlink" title="LENGTHOF操作符"></a><code>LENGTHOF</code>操作符</h4><p>计算数组中元素的个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">byte1    BYTE 10,20,30          ; LENGTHOF byte1 = 3</span><br><span class="line">array1   WORD 30 DUP(?),0,0     ;                  30+2</span><br><span class="line">array2   WORD 5 DUP(3 DUP(?))   ;                  5×3</span><br><span class="line">arrar3   DWORD 1,2,3,4          ;                  4</span><br><span class="line">digitStr BYTE &quot;12345678&quot;,0      ;                  9</span><br></pre></td></tr></table></figure><h4 id="SIZEOF操作符"><a href="#SIZEOF操作符" class="headerlink" title="SIZEOF操作符"></a><code>SIZEOF</code>操作符</h4><p>SIZEOF返回值 = LENGTHOF返回值 × TYPR返回值</p><h4 id="INVOKE伪指令"><a href="#INVOKE伪指令" class="headerlink" title="INVOKE伪指令"></a><code>INVOKE</code>伪指令</h4><p>自动在堆栈上压入参数并调用程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INVOKE Swap,ADDR Array,ADDR [Array+4]</span><br></pre></td></tr></table></figure><h4 id="PROTO伪指令"><a href="#PROTO伪指令" class="headerlink" title="PROTO伪指令"></a><code>PROTO</code>伪指令</h4><p>为一个已存在的过程创建原型。</p><p>原型声明了过程的名字和参数列表，允许一个过程在被定义之前就可以在其他地方被调用。</p><h3 id="三、数据传送指令"><a href="#三、数据传送指令" class="headerlink" title="三、数据传送指令"></a>三、数据传送指令</h3><h4 id="操作数类型"><a href="#操作数类型" class="headerlink" title="操作数类型"></a>操作数类型</h4><p>立即操作数（immediate）</p><ul><li><p>imm：8、16或32位立即数</p></li><li><p>imm8：8位立即数（字节）</p></li><li><p>imm16：16位立即数（字）</p></li><li><p>imm32： 32位立即数（双字）</p></li></ul><p>寄存器操作数（register）</p><ul><li><p>reg：任意的通用寄存器</p></li><li><p>sreg：16位段寄存器CS、DS、SS、ES、FS、GS</p></li><li><p>r8：AH、AL、BH、BL、CH、CL、DH、DL</p></li><li><p>16：AX、BX、CX、DX、SI、DI、SP、BP</p></li><li><p>r32：EAX、EBX、ECX、EDX、ESI、EDI、ESP、EBP</p></li></ul><p>内存操作数（memory）</p><ul><li>mem：8、16或32位内存操作数</li></ul><h4 id="直接内存操作数"><a href="#直接内存操作数" class="headerlink" title="直接内存操作数"></a>直接内存操作数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">...</span><br><span class="line">var1 BYTE 55h</span><br><span class="line">.code</span><br><span class="line">mov ax, @data</span><br><span class="line">mov ds, ax</span><br><span class="line">...</span><br><span class="line">mov al, [1040h]  ;设var1位于偏移1040h处</span><br><span class="line">mov al, var1     ;等价于 mov al, [var1]</span><br><span class="line">mov al, [var1+2] ;直接偏移操作数 </span><br></pre></td></tr></table></figure><h4 id="MOV指令"><a href="#MOV指令" class="headerlink" title="MOV指令"></a><code>MOV</code>指令</h4><ul><li>两个操作数的尺寸必须一致。</li><li><strong>两个操作数不能同时为内存操作数。</strong></li><li>目的操作数不能是CS，EIP和IP。</li><li>立即数不能直接送至段寄存器。</li></ul><p><img src="/2022/09/29/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86/image-20221005231757119.png" alt="image-20221005231757119"></p><h4 id="XCHG指令"><a href="#XCHG指令" class="headerlink" title="XCHG指令"></a><code>XCHG</code>指令</h4><p>交换两个操作数内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">;格式 不能直接交换两个内存操作数</span><br><span class="line">xchg reg, reg</span><br><span class="line">xchg reg, mem</span><br><span class="line">xchg mem, reg</span><br><span class="line"></span><br><span class="line">;交换两个内存操作数 利用寄存器</span><br><span class="line">mov ax, val1</span><br><span class="line">xchg ax, val2</span><br><span class="line">mov val1, ax</span><br></pre></td></tr></table></figure><h3 id="四、布尔和比较指令"><a href="#四、布尔和比较指令" class="headerlink" title="四、布尔和比较指令"></a>四、布尔和比较指令</h3><h4 id="1-AND指令"><a href="#1-AND指令" class="headerlink" title="1.AND指令"></a>1.<code>AND</code>指令</h4><p>按位与 两操作数大小必须相同 用于特定位置清0</p><p>将结果放在目标操作数中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">; 允许操作数形式：</span><br><span class="line">AND reg,reg</span><br><span class="line">AND reg,mem</span><br><span class="line">AND mem,reg</span><br><span class="line">AND reg,imm</span><br><span class="line">AND mem,imm</span><br><span class="line"></span><br><span class="line">; 大小写ASCII码</span><br><span class="line">&#x27;a&#x27; 61h 01100001</span><br><span class="line">&#x27;A&#x27; 41h 01000001</span><br></pre></td></tr></table></figure><ul><li>总是清除OF (溢出)和 CF(进位)</li><li>根据结果修改SF(符号)、ZF(零)、PF(奇偶)</li></ul><h4 id="2-OR指令"><a href="#2-OR指令" class="headerlink" title="2.OR指令"></a>2.<code>OR</code>指令</h4><p>按位或 用于特定位置置1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">; 将0-9间整数转换为对应ASCII码数字</span><br><span class="line">MOV dl,5    ;二进制值</span><br><span class="line">OR  dl,30h  ;00110000</span><br><span class="line">;或 add dl,30h</span><br><span class="line">;或 add dl,&#x27;0&#x27;</span><br></pre></td></tr></table></figure><h4 id="3-XOR指令"><a href="#3-XOR指令" class="headerlink" title="3.XOR指令"></a>3.<code>XOR</code>指令</h4><p>按位异或</p><h4 id="4-NOT指令"><a href="#4-NOT指令" class="headerlink" title="4.NOT指令"></a>4.<code>NOT</code>指令</h4><p>数据位按位取反，结果为反码</p><p>不影响任何标志位</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">;格式</span><br><span class="line">NOT reg</span><br><span class="line">NOT mem</span><br></pre></td></tr></table></figure><h4 id="5-TEST指令"><a href="#5-TEST指令" class="headerlink" title="5.TEST指令"></a>5.<code>TEST</code>指令</h4><p>格式同AND指令</p><p>两操作数按位相与，根据结果设置标志位，但不修改目的操作数</p><p>用于测试操作数某一位是0还是1</p><p>影响标志位：清除OF、CF   修改SF、ZF、PF</p><h4 id="6-CMP指令"><a href="#6-CMP指令" class="headerlink" title="6.CMP指令"></a>6.<code>CMP</code>指令</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">;格式同AND指令</span><br><span class="line">cmp 目的操作数, 源操作数</span><br><span class="line"></span><br><span class="line">;无符号操作数比较</span><br><span class="line">目的&lt;源  ZF=0 CF=1 SF≠OF</span><br><span class="line">目的&gt;源  ZF=0 CF=0 SF=OF</span><br><span class="line">目的=源  ZF=1 CF=0</span><br></pre></td></tr></table></figure><p>执行目的操作数-源操作数，但不回送结果，只影响标志位</p><p>根据相减结果修改OF、SF、ZF、CF、AF、PF</p><h4 id="7-设置和清除单个CPU标志"><a href="#7-设置和清除单个CPU标志" class="headerlink" title="7.设置和清除单个CPU标志"></a>7.设置和清除单个CPU标志</h4><h3 id="五、算术运算"><a href="#五、算术运算" class="headerlink" title="五、算术运算"></a>五、算术运算</h3><h4 id="INC和DNC指令"><a href="#INC和DNC指令" class="headerlink" title="INC和DNC指令"></a><code>INC</code>和<code>DNC</code>指令</h4><p>increment 即 +1</p><p>decrement 即 -1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">;格式</span><br><span class="line">inc reg/mem</span><br><span class="line">dec reg/mem</span><br><span class="line"></span><br><span class="line">inc BYTE PTR [si]</span><br></pre></td></tr></table></figure><p>INC和DEC指令不影响进位标志</p><h4 id="ADD指令"><a href="#ADD指令" class="headerlink" title="ADD指令"></a><code>ADD</code>指令</h4><p>格式同MOV指令</p><p><strong>两个操作数不能同时为内存操作数</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">var1 DWORD 10000h</span><br><span class="line">var2 DWORD 20000h</span><br><span class="line">.code</span><br><span class="line">mov eax,var1</span><br><span class="line">add eax,var2; 30000h</span><br></pre></td></tr></table></figure><h4 id="SUB指令"><a href="#SUB指令" class="headerlink" title="SUB指令"></a><code>SUB</code>指令</h4><p>格式同MOV指令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">var1 DWORD 30000h</span><br><span class="line">var2 DWORD 10000h</span><br><span class="line">.code</span><br><span class="line">mov eax,var1</span><br><span class="line">sub eax,var2; 20000h</span><br></pre></td></tr></table></figure><p>影响标志位：CF（进位）、ZF（零）、SF（符号）、OF（溢出）、AF（辅助进位）、PF（奇偶）</p><h4 id="NEG指令"><a href="#NEG指令" class="headerlink" title="NEG指令"></a><code>NEG</code>指令</h4><p>negate 求补 即求相反数</p><p>将操作数按位取反、末位加1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">;格式</span><br><span class="line">neg reg</span><br><span class="line">neg mem</span><br></pre></td></tr></table></figure><p>影响标志位同上</p><h4 id="mul乘法"><a href="#mul乘法" class="headerlink" title="mul乘法"></a><code>mul</code>乘法</h4><p>乘数与被乘数大小必须保持一致</p><p>被乘数AL(8位) -&gt; 积AX(16位)</p><p>被乘数AX(16位) -&gt; 积DX:AX</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">;8位无符号数乘法</span><br><span class="line">mov al, 5h   ;被乘数放al寄存器</span><br><span class="line">mov bl 10h   ;乘数任一寄存器 但不能直接乘立即数</span><br><span class="line">mul bl       ;cf = 0   积在AX中，50h</span><br><span class="line"></span><br><span class="line">;有符号乘法</span><br><span class="line">imul</span><br></pre></td></tr></table></figure><h4 id="div除法"><a href="#div除法" class="headerlink" title="div除法"></a><code>div</code>除法</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">;无符号数 8003h/100h</span><br><span class="line">mov dx,0   ;除数8位，则被除数16位放在AX，dx必须初始化</span><br><span class="line">mov ax,8003h  </span><br><span class="line">mov cx,100h   </span><br><span class="line">div cx     ;AX=0080h(商) DX=0003h(余数)</span><br><span class="line"></span><br><span class="line">;有符号除法</span><br><span class="line">idiv</span><br></pre></td></tr></table></figure><p>8位无符号除法  被除数在AX(高8位清零) 除数8位 商在AL 余数在AH</p><p>16位无符号除法  被除数在DX:AX(DX清零) 除数16位 商在AX 余数在DX</p><h3 id="六、控制转移-JMP和LOOP指令"><a href="#六、控制转移-JMP和LOOP指令" class="headerlink" title="六、控制转移 JMP和LOOP指令"></a>六、控制转移 <code>JMP</code>和<code>LOOP</code>指令</h3><p>控制转移分为：</p><ul><li>无条件转移：JMP</li><li>条件转移：LOOP</li></ul><h4 id="LOOP指令"><a href="#LOOP指令" class="headerlink" title="LOOP指令"></a><code>LOOP</code>指令</h4><p><code>ECX</code>用作循环计数器，实地址模式下<code>CX</code>用作循环计数器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    mov  ax,0</span><br><span class="line">    mov  ecx,5</span><br><span class="line">L1: inc  ax</span><br><span class="line">    loop L1</span><br></pre></td></tr></table></figure><h3 id="七、条件跳转指令"><a href="#七、条件跳转指令" class="headerlink" title="七、条件跳转指令"></a>七、条件跳转指令</h3><h4 id="基于特定CPU标志值的跳转指令"><a href="#基于特定CPU标志值的跳转指令" class="headerlink" title="基于特定CPU标志值的跳转指令"></a>基于特定<code>CPU</code>标志值的跳转指令</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">JZ/JE    ;为0则跳转 ZF=1</span><br><span class="line">JNZ/JNE  ;不为0则跳转 ZF=0</span><br><span class="line"></span><br><span class="line">JC       ;设进位标志则跳转CF=1</span><br><span class="line">JNC      ;未设 CF=0</span><br><span class="line"></span><br><span class="line">JO       ;设溢出标志则跳转OF=1</span><br><span class="line">JNO      ;未设OF=0</span><br><span class="line"></span><br><span class="line">JS       ;设符号标志则跳转SF=1</span><br><span class="line">JNS      ;未设SF=0</span><br><span class="line"></span><br><span class="line">JP       ;设奇偶标志则跳转PF=1</span><br><span class="line">JNP      ;未设PF=0</span><br></pre></td></tr></table></figure><h4 id="基于CMP指令的跳转"><a href="#基于CMP指令的跳转" class="headerlink" title="基于CMP指令的跳转"></a>基于<code>CMP</code>指令的跳转</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JE     ;相等则跳转</span><br><span class="line">JNE    ;不等则跳转</span><br><span class="line">JCXZ   ;CX=0则跳转</span><br><span class="line">JECXZ  ;ECX=0则跳转</span><br></pre></td></tr></table></figure><p>无符号整数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JA  ;大于则跳转 同JNBE不小于或等于则跳转</span><br><span class="line">JAE ;大于等于则跳转 同JNB不小于则跳转</span><br><span class="line">JB  ;小于则跳转 同JNAE不大于或等于则跳转</span><br><span class="line">JBE ;小于或等于则跳转 同JNA不大于则跳转</span><br></pre></td></tr></table></figure><p>有符号整数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JG  ;大于则跳转 同JNLE</span><br><span class="line">JGE ;大于等于 同JNL</span><br><span class="line">JL  ;小于 同JNGE</span><br><span class="line">JLE ;小于或等于 同JNG</span><br></pre></td></tr></table></figure><h4 id="LOOPZ与LOOPE"><a href="#LOOPZ与LOOPE" class="headerlink" title="LOOPZ与LOOPE"></a><code>LOOPZ</code>与<code>LOOPE</code></h4><p>LOOPZ与LOOPE等价</p><p>执行逻辑：</p><p>​    ECX=ECX-1</p><p>​    if ECX&gt;0 and ZF=1, jump to destination</p><h4 id="LOOPNZ与LOOPNE"><a href="#LOOPNZ与LOOPNE" class="headerlink" title="LOOPNZ与LOOPNE"></a><code>LOOPNZ</code>与<code>LOOPNE</code></h4><p>等价</p><p>执行逻辑：</p><p>​    ECX=ECX-1</p><p>​    if ECX&gt;0 and ZF=0, jump to destination</p><h3 id="八、移位指令"><a href="#八、移位指令" class="headerlink" title="八、移位指令"></a>八、移位指令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SHL/SHR    ;逻辑左/右移</span><br><span class="line">SAL/SAR    ;算数左/右移</span><br><span class="line">ROL/ROR    ;循环左/右移</span><br><span class="line">RCL/RCR    ;带进位的循环左/右移</span><br><span class="line">;SHLD/SHRD  双精度左/右移</span><br></pre></td></tr></table></figure><p>影响OF、CF</p><p>逻辑右移：最高位补0，最低位移入CF</p><p>算术右移：最高位保持，最低位移入CF</p><p>逻辑左移/算术左移：最高位移入CF，最低位补0</p><p>循环左移：最高位移入CF和最低位</p><p>循环右移：最低位移入CF和最高位</p><h3 id="九、过程"><a href="#九、过程" class="headerlink" title="九、过程"></a>九、过程</h3><p>用<code>PROC</code>和<code>ENDP</code>伪指令来声明</p><p>对程序进行逻辑划分为过程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">; 启动过程</span><br><span class="line">main PROC</span><br><span class="line">...</span><br><span class="line">call MySub</span><br><span class="line">...</span><br><span class="line">exit</span><br><span class="line">main ENDP</span><br><span class="line">; 其他过程</span><br><span class="line">MySub PROC</span><br><span class="line">...</span><br><span class="line">ret</span><br><span class="line">MySub ENDP</span><br></pre></td></tr></table></figure><p>局部标号和全局标号</p><h3 id="十、字符串和数组"><a href="#十、字符串和数组" class="headerlink" title="十、字符串和数组"></a>十、字符串和数组</h3><h4 id="基本指令"><a href="#基本指令" class="headerlink" title="基本指令"></a>基本指令</h4><div class="table-container"><table><thead><tr><th>指令</th><th>描述</th></tr></thead><tbody><tr><td>MOVSB, MOVSW, MOVSD</td><td><strong>移动字符串</strong>：拷贝DS:(E)SI寻址的内存操作数至ES:(E)DI</td></tr><tr><td>CMPSB, CMPSW, CMPSD</td><td><strong>比较字符串</strong>：比较内存中由DS:(E)SI寻址和ES:(E)DI寻址的字符串</td></tr><tr><td>SCASB, SCASW, SCASD</td><td><strong>扫描字符串</strong>：扫描ES:(E)DI指向的内存字符串查找与累加器匹配的值</td></tr><tr><td>STOSB, STOSW, STOSD</td><td><strong>存储字符串</strong>：将累加器内容存储到由ES:(E)DI寻址的内存中</td></tr><tr><td>LODSB, LODSW, LODSD</td><td><strong>将字符串数据装入累加器</strong>：将由DS:(E)SI寻址的内存单元装入累加其中</td></tr></tbody></table></div><ul><li>SI是DS段中的偏移</li><li>DI是ES段中的偏移</li></ul><h4 id="使用重复前缀"><a href="#使用重复前缀" class="headerlink" title="使用重复前缀"></a>使用重复前缀</h4><ul><li><p>字符串操作指令每次只能处理一个内存值。</p></li><li><p>通过增加一个重复前缀，字符串指令就会使用ECX作为计数器进行重复，实现用一条指令处理整个数组。</p></li></ul><div class="table-container"><table><thead><tr><th>前缀</th><th>描述</th></tr></thead><tbody><tr><td>REP</td><td>ECX&gt;0时重复</td></tr><tr><td>REPZ, REPE</td><td>ZF=1&amp;&amp;ECX&gt;0</td></tr><tr><td>REPNZ, REPNE</td><td>ZF=0&amp;&amp;ECX&gt;0</td></tr></tbody></table></div><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">main PROC</span><br><span class="line">  mov ax,@data  ;get addr of data seg</span><br><span class="line">  mov ds,ax   ;initialize DS</span><br><span class="line">  mov es,ax   ;initialize ES</span><br><span class="line"></span><br><span class="line">cld; clear direction flag</span><br><span class="line">mov si,OFFSET string1 ;SI points to source</span><br><span class="line">mov di,OFFSET string2 ;DI points to target</span><br><span class="line">mov cx,10    ; set counter to 10</span><br><span class="line">rep movsb; move 10 bytes</span><br></pre></td></tr></table></figure><p>方向标志：简单字符串指令使用方向标志来决定ESI和EDI是自动增加还是自动减少：</p><ul><li><p>方向标志位DF=0：ESI、EDI自动增加</p></li><li><p>DF=1：ESI、EDI自动减少</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CLD  ;清除方向标志</span><br><span class="line">STD  ;设置方向标志</span><br></pre></td></tr></table></figure></li></ul><h3 id="十一、结构"><a href="#十一、结构" class="headerlink" title="十一、结构"></a>十一、结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">;结构的定义</span><br><span class="line">Employee STRUCT</span><br><span class="line">   IdNum         BYTE  &quot;000000000&quot;</span><br><span class="line">   LastName      BYTE  30 DUP(0)</span><br><span class="line">   Years         WORD  0</span><br><span class="line">   SalaryHistory DWORD 0,0,0,0</span><br><span class="line">Employee ENDS</span><br><span class="line"></span><br><span class="line">;声明</span><br><span class="line">.data</span><br><span class="line">worker  Employee &lt;&gt;</span><br><span class="line">person1 Employee &lt;&quot;555223333&quot;&gt;</span><br><span class="line">person2 Employee &lt;,&quot;Jones&quot;&gt;</span><br><span class="line">person3 Employee &lt;,,,2 DUP(20000)&gt;</span><br><span class="line"></span><br><span class="line">;引用</span><br><span class="line">.code</span><br><span class="line">mov dx,worker.Years</span><br><span class="line">mov worker.SalaryHistory+4,30000  ; second salary</span><br><span class="line">mov edx,OFFSET worker.LastName</span><br><span class="line">mov esi,OFFSET worker</span><br><span class="line">mov ax,(Employee PTR [esi]).Years</span><br></pre></td></tr></table></figure><h3 id="十二、宏"><a href="#十二、宏" class="headerlink" title="十二、宏"></a>十二、宏</h3><p>命名的汇编语句块。调用时直接拷贝插入程序中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">;宏的定义</span><br><span class="line">mWriteStr MACRO string</span><br><span class="line">   push edx</span><br><span class="line">   mov  edx,OFFSET string</span><br><span class="line">   call WriteString</span><br><span class="line">   pop  edx</span><br><span class="line">ENDM</span><br><span class="line"></span><br><span class="line">;调用</span><br><span class="line">.data</span><br><span class="line">msg1 BYTE &quot;This is message 1.&quot;,0Dh,0Ah,’$’</span><br><span class="line">msg2 BYTE &quot;This is message 2.&quot;,0Dh,0Ah,’$’</span><br><span class="line">msg3 BYTE &quot;This is message 3.&quot;,0Dh,0Ah,’$’</span><br><span class="line">.code</span><br><span class="line">mWriteStr msg1</span><br><span class="line">mWriteStr msg2</span><br><span class="line">mWriteStr msg3</span><br></pre></td></tr></table></figure><h2 id="编程题举例"><a href="#编程题举例" class="headerlink" title="编程题举例"></a>编程题举例</h2><p>数组间接寻址</p><p>16位汇编：三个字相加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">.data</span><br><span class="line">arrayW WORD 1000h,2000h,3000h</span><br><span class="line">.code</span><br><span class="line">mov ax,@data</span><br><span class="line">mov ds,ax</span><br><span class="line">mov si,OFFSET arrayW</span><br><span class="line">mov ax,[si]</span><br><span class="line">add si,2</span><br><span class="line">add ax,[si]</span><br><span class="line">add si,2</span><br><span class="line">add ax,[si]</span><br></pre></td></tr></table></figure><p><code>P106 3.8</code> 在DATA为首地址的主存区域中存放100个无符号8位数，试编写程序找出其中最大的数，并将其放在KVFF存储单元中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MOV BX, SEG DATA</span><br><span class="line">MOV DS, BX</span><br><span class="line">MOV BX, OFFSET DATA</span><br><span class="line"></span><br><span class="line">;初始化</span><br><span class="line">MOV AL, [BX]</span><br><span class="line">INC BX</span><br><span class="line">MOV CX, 99</span><br><span class="line">AGIN:CMP AL, [BX]</span><br><span class="line">JAE NEXT</span><br><span class="line">MOV AL, [BX]</span><br><span class="line">NEXT:INC BX</span><br><span class="line">LOOP AGIN</span><br><span class="line">MOV KVFF, AL</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><code>P107 3.14</code> 试编写8086汇编程序，统计由主存40000H开始的16K个单元中所存放的字符“A”的个数，并将结果存放在DX中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">START:MOV DX, 40000H</span><br><span class="line">MOV DS, DX</span><br><span class="line">MOV CX, 16*1024</span><br><span class="line">MOV SI, 0</span><br><span class="line">MOV DX, 0</span><br><span class="line">FIND:MOV AL, [SI]</span><br><span class="line">CMP AL, &#x27;A&#x27;</span><br><span class="line">JNE NEXT   ;或JNZ</span><br><span class="line">INC DX</span><br><span class="line">NEXT:INC SI</span><br><span class="line">LOOP FIND</span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p> <a href="https://godbolt.org">https://godbolt.org</a></p></blockquote><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
        <tags>
            
            <tag> 汇编 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo test</title>
      <link href="/2022/08/05/hexo-test/"/>
      <url>/2022/08/05/hexo-test/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是一个引用</p><p>本文为从魏佬那儿引用来的梁佬的testpost</p></blockquote><span id="more"></span><hr><h1 id="hexo博客支持性测试"><a href="#hexo博客支持性测试" class="headerlink" title="hexo博客支持性测试"></a>hexo博客支持性测试</h1><h2 id="文字测试"><a href="#文字测试" class="headerlink" title="文字测试"></a>文字测试</h2><p><em>斜体</em>，<strong>粗体</strong>，<strong><em>粗斜体</em></strong>，<del>删除线</del>，<a href>超链接</a></p><p>嵌入代码：<code>MessageBoxW(hWnd, L&quot;Hello world!&quot;, L&quot;Tips&quot;, MB_ICONASTERISK)</code></p><p>标签：<span class="label success">成功</span> <span class="label warning">警告</span> <span class="label danger">危险</span> <span class="label primary">重要</span> <span class="label info">信息</span> <span class="label default">默认</span></p><blockquote class="blockquote-center">            <i class="fa fa-quote-left"></i>            <p>居中引用</p>            <i class="fa fa-quote-right"></i>          </blockquote><div class="note info">            <p><strong>提示</strong></p><p><code>MessageBoxW</code>用于显示一个对话框</p>          </div><h2 id="代码-公式测试"><a href="#代码-公式测试" class="headerlink" title="代码/公式测试"></a>代码/公式测试</h2><h3 id="c-代码"><a href="#c-代码" class="headerlink" title="c++代码"></a>c++代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Hello World!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="折叠的Lua代码"><a href="#折叠的Lua代码" class="headerlink" title="折叠的Lua代码"></a>折叠的Lua代码</h3><div class="spoiler collapsed">    <div class="spoiler-title">        code    </div>    <div class="spoiler-content">        <figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hello = <span class="literal">true</span></span><br><span class="line"><span class="keyword">for</span> k, _ <span class="keyword">in</span> <span class="built_in">_G</span> <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">print</span>(k)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>    </div></div><h3 id="LaTeX公式："><a href="#LaTeX公式：" class="headerlink" title="LaTeX公式："></a>LaTeX公式：</h3><script type="math/tex; mode=display">x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}</script><script type="math/tex; mode=display">A = \begin{bmatrix}    a_{11} & a_{12} & ...    & a_{1n} \\\\    a_{21} & a_{22} & ...    & a_{2n} \\\\     a_{31} & a_{22} & ...    & a_{3n} \\\\    \vdots & \vdots & \ddots & \vdots \\\\    a_{n1} & a_{n2} & ...    & a_{nn} \\\\ \end{bmatrix} , b = \begin{bmatrix}  b_{1}  \\\\  b_{2}  \\\\  b_{3}  \\\\  \vdots \\\\  b_{n}  \\\\\end{bmatrix}</script><p>行内公式：$ \int_{0}^{\pi}{\sin x \mathrm{d} x} = 2 $</p><p>$a \times b$</p><h2 id="组件测试"><a href="#组件测试" class="headerlink" title="组件测试"></a>组件测试</h2><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p>一些 <code>Hello World</code></p><div class="table-container"><table><thead><tr><th style="text-align:left">语言</th><th></th></tr></thead><tbody><tr><td style="text-align:left">C</td><td><code>puts(&quot;Hello World!&quot;);</code></td></tr><tr><td style="text-align:left">C++</td><td><code>std::cout &lt;&lt; &quot;Hello World!&quot;&lt;&lt; std::endl;</code></td></tr><tr><td style="text-align:left">Python</td><td><code>print(&quot;Hello World!&quot;)</code></td></tr><tr><td style="text-align:left">Go</td><td><code>fmt.Println(&quot;Hello World!&quot;)</code></td></tr><tr><td style="text-align:left">BatchScript</td><td><code>echo Hello World!</code></td></tr><tr><td style="text-align:left"><del>brainfuck</del></td><td><code>++++++++++[&gt;+++++++&gt;++++++++++&gt;+++&gt;+&lt;&lt;&lt;&lt;-]&gt;++.&gt;+.+++++++..+++.&gt;++.&lt;&lt;+++++++++++++++.&gt;.+++.------.--------.&gt;+.&gt;.</code></td></tr></tbody></table></div><h3 id="选项卡"><a href="#选项卡" class="headerlink" title="选项卡"></a>选项卡</h3><div class="tabs" id="解决方案"><ul class="nav-tabs"><li class="tab active"><a href="#解决方案-1">解决方案 1</a></li><li class="tab"><a href="#解决方案-2">解决方案 2</a></li><li class="tab"><a href="#解决方案-3">解决方案 3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="解决方案-1"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Code for solution #1 */</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="解决方案-2"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Code for solution #2 */</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="解决方案-3"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Code for solution #3 */</span></span><br></pre></td></tr></table></figure></div></div></div><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
        <tags>
            
            <tag> hexo test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/08/04/hello-world/"/>
      <url>/2022/08/04/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
